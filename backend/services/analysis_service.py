import logging
import multiprocessing
import cv2
from pathlib import Path
from typing import Any, Dict, List

from backend.services.audio_service import AudioService
from backend.services.video_service import SlideDetectionService
from backend.services.llm_service import LLMJudgeService

# –í–ê–ñ–ù–û: –ù–ï –∏–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º OcrService –∑–¥–µ—Å—å –Ω–∞ –≤–µ—Ä—Ö–Ω–µ–º —É—Ä–æ–≤–Ω–µ,
# —á—Ç–æ–±—ã –Ω–µ —Å–ø—Ä–æ–≤–æ—Ü–∏—Ä–æ–≤–∞—Ç—å –∑–∞–≥—Ä—É–∑–∫—É Paddle –≤ –æ—Å–Ω–æ–≤–Ω–æ–º –ø—Ä–æ—Ü–µ—Å—Å–µ.

logger = logging.getLogger(__name__)


def _ocr_worker_task(
    video_path: str, slides_metadata: List[Dict], language: str = "en"
) -> List[Dict]:
    """
    –≠—Ç–∞ —Ñ—É–Ω–∫—Ü–∏—è –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è –≤ –û–¢–î–ï–õ–¨–ù–û–ú –ø—Ä–æ—Ü–µ—Å—Å–µ.
    –ó–¥–µ—Å—å –±–µ–∑–æ–ø–∞—Å–Ω–æ –≥—Ä—É–∑–∏—Ç—å PaddleOCR, —Ç–∞–∫ –∫–∞–∫ Torch –∑–¥–µ—Å—å –Ω–µ—Ç.

    Args:
        video_path: Path to the video file
        slides_metadata: List of detected slides with frame_index and timestamp
        language: ISO 639-1 language code detected from audio (e.g., 'en', 'es', 'fr')
    """
    import logging

    # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –ª–æ–≥–∏ –¥–ª—è –¥–æ—á–µ—Ä–Ω–µ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞
    logging.basicConfig(level=logging.INFO)
    worker_logger = logging.getLogger("OCR_Worker")

    worker_logger.info(
        f"Worker started. Processing {len(slides_metadata)} slides with language={language}..."
    )

    results = []
    cap = None

    try:
        # --- FIX: –ó–∞–ø—Ä–µ—â–∞–µ–º Paddle –ø–µ—Ä–µ—Ö–≤–∞—Ç—ã–≤–∞—Ç—å —Å–∏—Å—Ç–µ–º–Ω—ã–µ —Å–∏–≥–Ω–∞–ª—ã (SIGTERM) ---
        import paddle

        paddle.disable_signal_handler()
        # -----------------------------------------------------------------------

        # 1. Lazy Import –≤–Ω—É—Ç—Ä–∏ –ø—Ä–æ—Ü–µ—Å—Å–∞
        from backend.services.ocr_service import OcrService

        ocr_service = OcrService(lang=language)

        # 2. –û—Ç–∫—Ä—ã–≤–∞–µ–º –≤–∏–¥–µ–æ (OpenCV –±–µ–∑–æ–ø–∞—Å–µ–Ω –≤ –º—É–ª—å—Ç–∏–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥–µ)
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            worker_logger.error("Failed to open video in worker")
            return []

        # 3. –ü—Ä–æ–±–µ–≥–∞–µ–º –ø–æ —Å–ø–∏—Å–∫—É –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Å–ª–∞–π–¥–æ–≤
        for slide in slides_metadata:
            frame_idx = slide["frame_index"]
            timestamp = slide["timestamp_sec"]

            # –ü—Ä—ã–≥–∞–µ–º –∫ –∫–∞–¥—Ä—É
            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
            ret, frame = cap.read()

            if ret:
                text = ocr_service.extract_text(frame)
                if text and len(text.strip()) > 3:
                    results.append(
                        {
                            "timestamp": timestamp,
                            "frame_index": frame_idx,
                            "ocr_text": text,
                        }
                    )
    except Exception as e:
        worker_logger.error(f"Worker crashed: {e}")
    finally:
        if cap:
            cap.release()

    worker_logger.info(f"Worker finished. Found text on {len(results)} slides.")
    return results


class AnalysisService:
    """
    Coordinator service that runs the full multimodal analysis pipeline.
    """

    def __init__(self):
        # –í –æ—Å–Ω–æ–≤–Ω–æ–º –ø—Ä–æ—Ü–µ—Å—Å–µ –∂–∏–≤–µ—Ç —Ç–æ–ª—å–∫–æ Whisper (Torch) –∏ C++ –¥–µ—Ç–µ–∫—Ç–æ—Ä
        self.audio_service = AudioService(model_size="base")
        self.video_service = SlideDetectionService(
            min_scene_duration=2.0, min_area_ratio=0.15
        )
        self.llm_service = LLMJudgeService()

    def analyze_content(self, video_path: str) -> Dict[str, Any]:
        path = Path(video_path)
        if not path.exists():
            raise FileNotFoundError(f"Video not found: {video_path}")

        logger.info("üöÄ Starting analysis for: %s", video_path)

        # --- Phase 1: Audio Processing (Main Process) ---
        # Whisper —Ä–∞–±–æ—Ç–∞–µ—Ç –∑–¥–µ—Å—å
        logger.info("üéß Phase 1: Audio Processing...")
        transcript = []
        detected_language = "en"  # Default fallback
        try:
            audio_file = self.audio_service.extract_audio(video_path)
            transcript, detected_language = self.audio_service.transcribe(audio_file)
            logger.info(
                f"‚úÖ Transcribed {len(transcript)} segments in {detected_language}"
            )
        except Exception as e:
            logger.error(f"Audio processing failed: {e}")

        # --- Phase 2: Visual Processing (C++ Detection) ---
        # C++ —Ä–∞–±–æ—Ç–∞–µ—Ç –∑–¥–µ—Å—å (–±—ã—Å—Ç—Ä–æ –∏ –±–µ–∑ –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤)
        logger.info("üëÅÔ∏è Phase 2: Visual Processing (Detection)...")
        detected_slides = []
        try:
            detected_slides = self.video_service.process_video(video_path)
            logger.info(f"‚ö° C++ detected {len(detected_slides)} keyframes")
        except Exception as e:
            logger.error(f"Slide detection failed: {e}")

        # --- Phase 3: OCR (Isolated Process) ---
        # –ó–∞–ø—É—Å–∫–∞–µ–º Paddle –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–π "–ø–µ—Å–æ—á–Ω–∏—Ü–µ"
        logger.info(
            f"üìñ Phase 3: OCR Extraction (Isolated, lang={detected_language})..."
        )
        visual_data = []
        if detected_slides:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º 'spawn', —á—Ç–æ–±—ã –ø—Ä–æ—Ü–µ—Å—Å –±—ã–ª —á–∏—Å—Ç—ã–º (–±–µ–∑ Torch –≤ –ø–∞–º—è—Ç–∏)
            ctx = multiprocessing.get_context("spawn")
            with ctx.Pool(processes=1) as pool:
                # –ü–µ—Ä–µ–¥–∞–µ–º –ø—É—Ç—å –∫ –≤–∏–¥–µ–æ, —Å–ø–∏—Å–æ–∫ –∫–∞–¥—Ä–æ–≤, –∏ detected language
                visual_data = pool.apply(
                    _ocr_worker_task, (video_path, detected_slides, detected_language)
                )

        # –°–æ–±–∏—Ä–∞–µ–º —Å—ã—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ
        analysis_result = {
            "meta": {
                "video_path": str(video_path),
                "status": "completed",
                "detected_language": detected_language,
            },
            "transcription": transcript,
            "visual_context": visual_data,
        }

        # --- [NEW] Phase 4: LLM Evaluation ---
        logger.info("üß† Phase 4: LLM Evaluation...")
        # –ü–µ—Ä–µ–¥–∞–µ–º —Å–æ–±—Ä–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (—Ç–µ–∫—Å—Ç + —Å–ª–∞–π–¥—ã) –≤ Gemini
        ai_feedback = self.llm_service.evaluate_interview(analysis_result)

        # –î–æ–±–∞–≤–ª—è–µ–º –æ—Ü–µ–Ω–∫—É –≤ —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç
        analysis_result["ai_evaluation"] = ai_feedback

        return analysis_result
